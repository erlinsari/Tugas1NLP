{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Analisis Sentimen Ulasan DANA\n",
                "## NLP Pipeline \u2014 Data Acquisition sampai Evaluation\n",
                "\n",
                "Notebook ini mengimplementasikan NLP Pipeline lengkap menggunakan ulasan aplikasi\n",
                "**DANA - Dompet Digital Indonesia** dari Google Play Store.\n",
                "\n",
                "**Tahapan:**\n",
                "1. Data Acquisition\n",
                "2. Text Cleaning & Pre-processing\n",
                "3. Feature Engineering (TF-IDF)\n",
                "4. Modeling (SVM / LinearSVC)\n",
                "5. Evaluation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install google-play-scraper pandas numpy scikit-learn matplotlib seaborn wordcloud PySastrawi nltk joblib -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from wordcloud import WordCloud\n",
                "\n",
                "import nltk\n",
                "nltk.download(\"punkt\", quiet=True)\n",
                "nltk.download(\"punkt_tab\", quiet=True)\n",
                "nltk.download(\"stopwords\", quiet=True)\n",
                "\n",
                "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
                "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.calibration import CalibratedClassifierCV\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import joblib\n",
                "\n",
                "print(\"Semua library berhasil dimuat!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Tahap 1 \u2014 Data Acquisition\n",
                "\n",
                "Mengambil ulasan aplikasi DANA dari Google Play Store menggunakan library `google-play-scraper`.\n",
                "Label sentimen ditentukan berdasarkan rating bintang:\n",
                "- Bintang 4\u20135 \u2192 **Positif**\n",
                "- Bintang 3 \u2192 **Netral**\n",
                "- Bintang 1\u20132 \u2192 **Negatif**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google_play_scraper import reviews, Sort\n",
                "\n",
                "APP_ID = \"id.dana\"\n",
                "TARGET = 2000\n",
                "\n",
                "semua_ulasan = []\n",
                "token = None\n",
                "\n",
                "try:\n",
                "    while len(semua_ulasan) < TARGET:\n",
                "        sisa = TARGET - len(semua_ulasan)\n",
                "        batch = min(200, sisa)\n",
                "\n",
                "        hasil, token = reviews(\n",
                "            APP_ID,\n",
                "            lang=\"id\",\n",
                "            country=\"id\",\n",
                "            sort=Sort.NEWEST,\n",
                "            count=batch,\n",
                "            continuation_token=token\n",
                "        )\n",
                "\n",
                "        if not hasil:\n",
                "            break\n",
                "\n",
                "        semua_ulasan.extend(hasil)\n",
                "        print(f\"Mengambil ulasan: {len(semua_ulasan)}/{TARGET}\")\n",
                "\n",
                "        if token is None:\n",
                "            break\n",
                "\n",
                "        time.sleep(1)\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")\n",
                "\n",
                "print(f\"\\nTotal ulasan berhasil diambil: {len(semua_ulasan)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"data\", exist_ok=True)\n",
                "\n",
                "daftar = []\n",
                "for u in semua_ulasan:\n",
                "    daftar.append({\n",
                "        \"id_ulasan\": u.get(\"reviewId\", \"\"),\n",
                "        \"nama_pengguna\": u.get(\"userName\", \"\"),\n",
                "        \"isi_ulasan\": u.get(\"content\", \"\"),\n",
                "        \"bintang\": u.get(\"score\", 0),\n",
                "        \"tanggal_ulasan\": u.get(\"at\", \"\"),\n",
                "        \"jumlah_like\": u.get(\"thumbsUpCount\", 0),\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(daftar)\n",
                "\n",
                "def tentukan_sentimen(bintang):\n",
                "    if bintang >= 4:\n",
                "        return \"Positif\"\n",
                "    elif bintang == 3:\n",
                "        return \"Netral\"\n",
                "    else:\n",
                "        return \"Negatif\"\n",
                "\n",
                "df[\"sentimen\"] = df[\"bintang\"].apply(tentukan_sentimen)\n",
                "df = df[df[\"isi_ulasan\"].str.strip() != \"\"]\n",
                "df = df.dropna(subset=[\"isi_ulasan\"])\n",
                "\n",
                "df.to_csv(\"data/ulasan_dana_mentah.csv\", index=False, encoding=\"utf-8-sig\")\n",
                "print(f\"Data disimpan: {len(df)} ulasan\")\n",
                "print(\"\\nDistribusi Sentimen:\")\n",
                "print(df[\"sentimen\"].value_counts())\n",
                "df.head(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Tahap 2 \u2014 Text Cleaning & Pre-processing\n",
                "\n",
                "Pipeline pembersihan teks:\n",
                "1. Lowercase\n",
                "2. Hapus URL, mention, hashtag\n",
                "3. Hapus emoji dan karakter non-ASCII\n",
                "4. Hapus angka\n",
                "5. Hapus tanda baca\n",
                "6. Normalisasi kata slang/gaul\n",
                "7. Stopword removal (PySastrawi)\n",
                "8. Stemming (PySastrawi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Memuat Stemmer & StopWord Bahasa Indonesia...\")\n",
                "hapus_sw = StopWordRemoverFactory().create_stop_word_remover()\n",
                "stemmer = StemmerFactory().create_stemmer()\n",
                "\n",
                "kamus_slang = {\n",
                "    \"gk\": \"tidak\", \"ga\": \"tidak\", \"gak\": \"tidak\", \"g\": \"tidak\",\n",
                "    \"ngga\": \"tidak\", \"nggak\": \"tidak\", \"kagak\": \"tidak\",\n",
                "    \"yg\": \"yang\", \"yng\": \"yang\", \"drpd\": \"daripada\",\n",
                "    \"dgn\": \"dengan\", \"dg\": \"dengan\", \"sm\": \"sama\",\n",
                "    \"sy\": \"saya\", \"gue\": \"saya\", \"gw\": \"saya\",\n",
                "    \"lo\": \"kamu\", \"lu\": \"kamu\", \"km\": \"kamu\",\n",
                "    \"sdh\": \"sudah\", \"udh\": \"sudah\", \"udah\": \"sudah\",\n",
                "    \"blm\": \"belum\", \"blom\": \"belum\", \"blum\": \"belum\",\n",
                "    \"dpt\": \"dapat\", \"bsa\": \"bisa\", \"bs\": \"bisa\",\n",
                "    \"hrs\": \"harus\", \"krn\": \"karena\", \"karna\": \"karena\",\n",
                "    \"tp\": \"tapi\", \"tpi\": \"tapi\", \"ttg\": \"tentang\",\n",
                "    \"dl\": \"dulu\", \"dlu\": \"dulu\",\n",
                "    \"bnyk\": \"banyak\", \"byk\": \"banyak\",\n",
                "    \"msh\": \"masih\", \"masi\": \"masih\",\n",
                "    \"bgt\": \"sangat\", \"banget\": \"sangat\",\n",
                "    \"aja\": \"saja\", \"aj\": \"saja\",\n",
                "    \"jg\": \"juga\", \"spt\": \"seperti\",\n",
                "    \"lg\": \"lagi\", \"klo\": \"kalau\", \"kalo\": \"kalau\",\n",
                "    \"dr\": \"dari\", \"utk\": \"untuk\", \"u\": \"untuk\",\n",
                "    \"tdk\": \"tidak\", \"pd\": \"pada\",\n",
                "    \"abis\": \"habis\", \"lbh\": \"lebih\", \"lbih\": \"lebih\",\n",
                "    \"org\": \"orang\", \"emg\": \"memang\", \"emang\": \"memang\",\n",
                "    \"knp\": \"kenapa\", \"tau\": \"tahu\",\n",
                "    \"jgn\": \"jangan\", \"aplikasinya\": \"aplikasi\", \"appnya\": \"aplikasi\",\n",
                "}\n",
                "\n",
                "def normalisasi_slang(teks):\n",
                "    return \" \".join([kamus_slang.get(k, k) for k in teks.split()])\n",
                "\n",
                "def bersihkan_teks(teks):\n",
                "    if pd.isna(teks) or str(teks).strip() == \"\":\n",
                "        return \"\"\n",
                "    teks = str(teks).lower()\n",
                "    teks = re.sub(r\"http\\S+|www\\.\\S+\", \"\", teks)\n",
                "    teks = re.sub(r\"@\\w+|#\\w+\", \"\", teks)\n",
                "    teks = teks.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
                "    teks = re.sub(r\"\\d+\", \"\", teks)\n",
                "    teks = re.sub(r\"[^\\w\\s]\", \" \", teks)\n",
                "    teks = re.sub(r\"\\s+\", \" \", teks).strip()\n",
                "    teks = normalisasi_slang(teks)\n",
                "    teks = hapus_sw.remove(teks)\n",
                "    teks = stemmer.stem(teks)\n",
                "    return teks.strip()\n",
                "\n",
                "print(\"Fungsi pre-processing siap.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Memproses teks... (mungkin beberapa menit)\")\n",
                "total = len(df)\n",
                "hasil_bersih = []\n",
                "\n",
                "for i, baris in df.iterrows():\n",
                "    hasil_bersih.append(bersihkan_teks(baris[\"isi_ulasan\"]))\n",
                "    if (i + 1) % 200 == 0:\n",
                "        print(f\"  {i + 1}/{total} selesai...\")\n",
                "\n",
                "df[\"teks_bersih\"] = hasil_bersih\n",
                "df = df[df[\"teks_bersih\"].str.strip() != \"\"]\n",
                "df = df.dropna(subset=[\"teks_bersih\"])\n",
                "\n",
                "df.to_csv(\"data/ulasan_dana_bersih.csv\", index=False, encoding=\"utf-8-sig\")\n",
                "print(f\"\\nSelesai. {len(df)} baris disimpan.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Contoh hasil pre-processing\n",
                "print(\"Contoh hasil pre-processing:\\n\")\n",
                "for _, row in df.head(5).iterrows():\n",
                "    asli = str(row[\"isi_ulasan\"])[:100]\n",
                "    bersih = str(row[\"teks_bersih\"])[:100]\n",
                "    print(f\"SEBELUM  : {asli}\")\n",
                "    print(f\"SESUDAH  : {bersih}\")\n",
                "    print(f\"SENTIMEN : {row['sentimen']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Tahap 3 \u2014 Feature Engineering, Modeling & Evaluation\n",
                "\n",
                "- **Feature Engineering:** TF-IDF Vectorizer (6.000 fitur, unigram + bigram)\n",
                "- **Modeling:** SVM (LinearSVC) dengan CalibratedClassifierCV\n",
                "- **Class Weight:** Balanced (menangani ketidakseimbangan kelas)\n",
                "- **Split data:** 80% latih, 20% uji (stratified)\n",
                "- **Evaluasi:** Accuracy, Precision, Recall, F1-Score, Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"model\", exist_ok=True)\n",
                "os.makedirs(\"gambar\", exist_ok=True)\n",
                "\n",
                "df = pd.read_csv(\"data/ulasan_dana_bersih.csv\")\n",
                "df = df.dropna(subset=[\"teks_bersih\", \"sentimen\"])\n",
                "df = df[df[\"teks_bersih\"].str.strip() != \"\"]\n",
                "\n",
                "distribusi = df[\"sentimen\"].value_counts()\n",
                "print(f\"Data: {len(df)} baris\\n\")\n",
                "print(\"Distribusi Sentimen:\")\n",
                "print(distribusi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[\"teks_bersih\"]\n",
                "y = df[\"sentimen\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "vektorizer = TfidfVectorizer(\n",
                "    max_features=6000,\n",
                "    ngram_range=(1, 2),\n",
                "    min_df=2,\n",
                "    max_df=0.95,\n",
                "    sublinear_tf=True,\n",
                ")\n",
                "\n",
                "X_train_tfidf = vektorizer.fit_transform(X_train)\n",
                "X_test_tfidf = vektorizer.transform(X_test)\n",
                "\n",
                "print(f\"Data latih : {len(X_train)} sampel\")\n",
                "print(f\"Data uji   : {len(X_test)} sampel\")\n",
                "print(f\"Jumlah fitur TF-IDF: {X_train_tfidf.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Melatih model SVM...\")\n",
                "\n",
                "svm = LinearSVC(C=1.0, max_iter=5000, random_state=42, class_weight=\"balanced\")\n",
                "model = CalibratedClassifierCV(svm, cv=3)\n",
                "model.fit(X_train_tfidf, y_train)\n",
                "print(\"Model selesai dilatih.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Evaluasi Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prediksi = model.predict(X_test_tfidf)\n",
                "akurasi = accuracy_score(y_test, prediksi)\n",
                "label_unik = sorted(y.unique())\n",
                "\n",
                "print(f\"Akurasi: {akurasi * 100:.2f}%\")\n",
                "print(\"\\nLaporan Klasifikasi:\")\n",
                "print(classification_report(y_test, prediksi, target_names=label_unik))\n",
                "\n",
                "laporan = classification_report(y_test, prediksi, target_names=label_unik, output_dict=True)\n",
                "pd.DataFrame(laporan).transpose().to_csv(\"data/laporan_evaluasi.csv\", encoding=\"utf-8-sig\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualisasi \u2014 Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_test, prediksi, labels=label_unik)\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"RdYlGn\",\n",
                "            xticklabels=label_unik, yticklabels=label_unik,\n",
                "            linewidths=0.5, ax=ax)\n",
                "ax.set_title(\"Confusion Matrix - Sentimen Ulasan DANA\", fontsize=13, fontweight=\"bold\", pad=15)\n",
                "ax.set_xlabel(\"Prediksi\", fontsize=11)\n",
                "ax.set_ylabel(\"Aktual\", fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"gambar/confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualisasi \u2014 Distribusi Sentimen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "palet = {\"Positif\": \"#10b981\", \"Netral\": \"#f59e0b\", \"Negatif\": \"#ef4444\"}\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "fig.suptitle(\"Distribusi Sentimen Ulasan DANA\", fontsize=14, fontweight=\"bold\")\n",
                "\n",
                "warna = [palet.get(k, \"#6b7280\") for k in distribusi.index]\n",
                "batang = ax1.bar(distribusi.index, distribusi.values, color=warna, edgecolor=\"white\", linewidth=1.5)\n",
                "ax1.set_title(\"Jumlah Ulasan per Sentimen\")\n",
                "ax1.set_xlabel(\"Sentimen\")\n",
                "ax1.set_ylabel(\"Jumlah\")\n",
                "for p in batang:\n",
                "    ax1.annotate(f\"{p.get_height():,}\",\n",
                "                 (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                 ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
                "\n",
                "ax2.pie(distribusi.values, labels=distribusi.index, colors=warna,\n",
                "        autopct=\"%1.1f%%\", startangle=90,\n",
                "        wedgeprops=dict(edgecolor=\"white\", linewidth=2))\n",
                "ax2.set_title(\"Persentase Sentimen\")\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"gambar/distribusi_sentimen.png\", dpi=150, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualisasi \u2014 Word Cloud per Sentimen"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, sumbu = plt.subplots(1, 3, figsize=(18, 5))\n",
                "fig.suptitle(\"Word Cloud Ulasan DANA per Sentimen\", fontsize=14, fontweight=\"bold\")\n",
                "\n",
                "wc_cfg = {\n",
                "    \"Positif\": {\"cmap\": \"Greens\", \"ax\": sumbu[0]},\n",
                "    \"Netral\":  {\"cmap\": \"Oranges\", \"ax\": sumbu[1]},\n",
                "    \"Negatif\": {\"cmap\": \"Reds\",    \"ax\": sumbu[2]},\n",
                "}\n",
                "\n",
                "for label, cfg in wc_cfg.items():\n",
                "    teks = \" \".join(df[df[\"sentimen\"] == label][\"teks_bersih\"].tolist())\n",
                "    if teks.strip():\n",
                "        wc = WordCloud(width=500, height=300, background_color=\"white\",\n",
                "                       colormap=cfg[\"cmap\"], max_words=80,\n",
                "                       collocations=False).generate(teks)\n",
                "        cfg[\"ax\"].imshow(wc, interpolation=\"bilinear\")\n",
                "    cfg[\"ax\"].set_title(f\"Sentimen: {label}\", fontsize=12, fontweight=\"bold\")\n",
                "    cfg[\"ax\"].axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"gambar/wordcloud_sentimen.png\", dpi=150, bbox_inches=\"tight\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Simpan Model & Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(model, \"model/model_sentimen.pkl\")\n",
                "joblib.dump(vektorizer, \"model/vektorizer_tfidf.pkl\")\n",
                "\n",
                "metadata = {\n",
                "    \"akurasi\": float(akurasi),\n",
                "    \"jumlah_data\": len(df),\n",
                "    \"jumlah_latih\": len(X_train),\n",
                "    \"jumlah_uji\": len(X_test),\n",
                "    \"jumlah_fitur\": int(X_train_tfidf.shape[1]),\n",
                "    \"kelas\": label_unik,\n",
                "    \"distribusi\": distribusi.to_dict(),\n",
                "    \"algoritma\": \"SVM (LinearSVC + CalibratedClassifierCV)\",\n",
                "}\n",
                "for s in label_unik:\n",
                "    if s in laporan:\n",
                "        metadata[f\"presisi_{s.lower()}\"] = laporan[s].get(\"precision\", 0)\n",
                "        metadata[f\"recall_{s.lower()}\"] = laporan[s].get(\"recall\", 0)\n",
                "        metadata[f\"f1_{s.lower()}\"] = laporan[s].get(\"f1-score\", 0)\n",
                "\n",
                "with open(\"model/metadata_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(\"Model  -> model/model_sentimen.pkl\")\n",
                "print(\"Vektor -> model/vektorizer_tfidf.pkl\")\n",
                "print(\"Meta   -> model/metadata_model.json\")\n",
                "print(f\"\\nAkurasi akhir: {akurasi * 100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Ringkasan\n",
                "\n",
                "| Aspek | Detail |\n",
                "|-------|--------|\n",
                "| **Sumber Data** | Ulasan DANA dari Google Play Store |\n",
                "| **Jumlah Data** | ~2.000 ulasan |\n",
                "| **Pelabelan** | Berdasarkan rating bintang (4-5: Positif, 3: Netral, 1-2: Negatif) |\n",
                "| **Pre-processing** | Lowercase, Hapus URL/Mention/Emoji, Normalisasi Slang, Stopword, Stemming |\n",
                "| **Feature Engineering** | TF-IDF Vectorizer (6.000 fitur, unigram+bigram) |\n",
                "| **Model** | SVM (LinearSVC + CalibratedClassifierCV) |\n",
                "| **Deployment** | Streamlit Web App |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}